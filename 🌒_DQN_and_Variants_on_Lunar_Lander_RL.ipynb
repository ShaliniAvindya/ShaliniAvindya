{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "üåí DQN and Variants on Lunar Lander - RL",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAvindya/ShaliniAvindya/blob/main/%F0%9F%8C%92_DQN_and_Variants_on_Lunar_Lander_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Learning and Variants in Gym's Lunar Lander Environment\n",
        "\n",
        "In this notebook, we will explore the implementation of a Deep Q-Learning (DQN) agent to navigate Gym's Lunar Lander environment.\n",
        "\n",
        "We will use apply four variants of the DQN algorithm:\n",
        "- The Classic DQN (Mihn et al 2013)\n",
        "- Double DQN (Hasselt et al 2015)\n",
        "- Dueling DQN (Wang et al 2015)\n",
        "- Double Dueling DQN\n",
        "\n",
        "In the Lunar Lander environment, the agent's task is to learn how to land a lunar module safely on the moon's surface. This requires the agent to balance fuel efficiency and safety considerations. The agent needs to learn from its past experiences, developing a strategy to approach the landing pad while minimizing its speed and using as little fuel as possible.\n",
        "\n",
        "All reinforcement learning (RL) methods will be built from scratch, providing a comprehensive understanding of their workings and we will use PyTorch to build our neural network model.\n",
        "\n",
        "Let's initialize a LunarLander-v2 environmnet, make random actions in the environment, then view a recording of it."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-06-21T19:20:26.339215Z",
          "iopub.execute_input": "2023-06-21T19:20:26.339632Z",
          "iopub.status.idle": "2023-06-21T19:20:26.345129Z",
          "shell.execute_reply.started": "2023-06-21T19:20:26.3396Z",
          "shell.execute_reply": "2023-06-21T19:20:26.343795Z"
        },
        "id": "rTCnCfSGiNKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To use in Kaggle we need to install these two packages\n",
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:00:24.279655Z",
          "iopub.execute_input": "2023-07-01T14:00:24.280022Z",
          "iopub.status.idle": "2023-07-01T14:01:34.147896Z",
          "shell.execute_reply.started": "2023-07-01T14:00:24.279992Z",
          "shell.execute_reply": "2023-07-01T14:01:34.146629Z"
        },
        "trusted": true,
        "id": "0V_WDmMiiNKX",
        "outputId": "2893c3bf-a12d-4016-9e75-0b5db5934809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m1.8/1.9 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.2.1)\n",
            "Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376099 sha256=c42521eefd9efbcaffb5b5c9a17df0681bc97b1c8de421460338284c4753210a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.0\n",
            "    Uninstalling pygame-2.6.0:\n",
            "      Successfully uninstalled pygame-2.6.0\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "obs = env.reset(seed=42)\n",
        "\n",
        "# Play one complete episode with random actions\n",
        "terminated, truncated = False, False\n",
        "while not (terminated or truncated):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated = env.step(action)  # Unpacking 4 values\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:01.506158Z",
          "iopub.execute_input": "2023-07-01T14:03:01.507301Z",
          "iopub.status.idle": "2023-07-01T14:03:01.993381Z",
          "shell.execute_reply.started": "2023-07-01T14:03:01.50725Z",
          "shell.execute_reply": "2023-07-01T14:03:01.992363Z"
        },
        "trusted": true,
        "id": "TsJwvjZciNKb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/tQ3zeQA.gif)\n",
        "\n",
        "## General Information\n",
        "This information is from the official Gym documentation.\n",
        "\n",
        "https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
        "\n",
        "| Feature Category  | Details                                |\n",
        "|-------------------|----------------------------------------|\n",
        "| Action Space      | Discrete(4)                            |\n",
        "| Observation Shape | (8,)                                   |\n",
        "| Observation High  | [1.5 1.5 5. 5. 3.14 5. 1. 1. ]         |\n",
        "| Observation Low   | [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] |\n",
        "| Import            | `gym.make(\"LunarLander-v2\")`           |\n",
        "\n",
        "## Description of Environment\n",
        "\n",
        "This environment is a classic rocket trajectory optimization problem. According to Pontryagin‚Äôs maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
        "\n",
        "There are two environment versions: discrete or continuous. The landing pad is always at coordinates `(0,0)`. The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent could learn to fly and then land on its first attempt.\n",
        "\n",
        "## Action Space\n",
        "There are four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
        "\n",
        "| Action  | Result                          |\n",
        "|---------|---------------------------------|\n",
        "| 0       | Do nothing                      |\n",
        "| 1       | Fire left orientation engine    |\n",
        "| 2       | Fire main engine                |\n",
        "| 3       | Fire right orientation engine   |\n",
        "\n",
        "## Observation Space\n",
        "The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear velocities in `x` & `y`, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
        "\n",
        "| Observation  | Value                                   |\n",
        "|--------------|-----------------------------------------|\n",
        "| 0            | `x` coordinate (float)                  |\n",
        "| 1            | `y` coordinate (float)                  |\n",
        "| 2            | `x` linear velocity (float)             |\n",
        "| 3            | `y` linear velocity (float)             |\n",
        "| 4            | Angle in radians from -œÄ to +œÄ (float)  |\n",
        "| 5            | Angular velocity (float)                |\n",
        "| 6            | Left leg contact (bool)                 |\n",
        "| 7            | Right leg contact (bool)                |\n",
        "\n",
        "## Rewards\n",
        "Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points.\n",
        "\n",
        "## Starting State\n",
        "The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
        "\n",
        "## Episode Termination\n",
        "The episode finishes if:\n",
        "\n",
        "1. The lander crashes (the lander body gets in contact with the moon);\n",
        "\n",
        "2. The lander gets outside of the viewport (`x` coordinate is greater than 1);\n",
        "\n",
        "3. The lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn‚Äôt move and doesn‚Äôt collide with any other body:\n",
        "\n",
        "---\n",
        "\n",
        "## The Safe Agent\n",
        "We're going to implement a simple agent 'The Safe Agent' who will thrust upward if and only if the lander's `y` position is less than 0.5.\n",
        "\n",
        "In theory this agent shouldn't hit the ground as we have unlimited fuel, but let's see."
      ],
      "metadata": {
        "id": "s7MAm_1biNKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SafeAgent:\n",
        "    '''\n",
        "    An agent that will simply fly upward if the lander gets too close to the ground.\n",
        "    '''\n",
        "    def act(self, state):\n",
        "        '''\n",
        "        Decision making method.\n",
        "        Fly up if below the minimum height.\n",
        "        '''\n",
        "        MIN_HEIGHT = 1\n",
        "\n",
        "        if state[1] < MIN_HEIGHT:\n",
        "            return 2  # Action to fire the main engine (fly up)\n",
        "        else:\n",
        "            return 0  # No action (do nothing)\n",
        "\n",
        "def play_episode(env, agent, seed=42):\n",
        "    '''\n",
        "    Plays a full episode for a given agent, environment, and seed.\n",
        "    '''\n",
        "    score = 0\n",
        "    state = env.reset(seed=seed)  # Adjusted to handle a single return value\n",
        "\n",
        "    while True:\n",
        "        action = agent.act(state)\n",
        "        state, reward, terminated, truncated = env.step(action)  # Unpacking 4 values\n",
        "        done = terminated or truncated\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        # End the episode if done\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return score\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "agent = SafeAgent()\n",
        "\n",
        "# Play an episode and print the score\n",
        "score = play_episode(env, agent)\n",
        "print(f\"Episode score: {score}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:07.535676Z",
          "iopub.execute_input": "2023-07-01T14:03:07.536053Z",
          "iopub.status.idle": "2023-07-01T14:03:07.576873Z",
          "shell.execute_reply.started": "2023-07-01T14:03:07.536021Z",
          "shell.execute_reply": "2023-07-01T14:03:07.575868Z"
        },
        "trusted": true,
        "id": "Vjne837JiNKf",
        "outputId": "4539e8e5-b486-4d67-ccfd-73c9af248ea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode score: -260.82660246093013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/qFNn9ai.gif)\n",
        "\n",
        "#### Observations:\n",
        "- The safe agent may not have hit the ground, but it didn't take long to fly off screen, due to its inability to use the side engines.\n",
        "\n",
        "---\n",
        "\n",
        "## The Stable Agent\n",
        "Let's try to define and agent that can remain stable in the air.\n",
        "\n",
        "It will operate via the following rules:\n",
        "\n",
        "1. If below height of 1: action = 2 (main engine)\n",
        "2. If angle is above œÄ/50: action = 1 (fire right engine)\n",
        "3. If angle is above œÄ/50: action = 1 (fire left engine)\n",
        "4. If x distance is above 0.4: action = 3 (fire left engine)\n",
        "5. If x distance is below -0.4: action = 1 (fire left engine)\n",
        "6. If below height of 1.5: action = 2 (main engine)\n",
        "6. Else: action = 0 (do nothing)\n",
        "\n",
        "The idea is the lander will always use its main engine if it falls below a certain height, next it will prioritize stabilizing the angle of the lander, then the distance, then keeping it above another height.\n",
        "\n",
        "Let's see how this approach does:"
      ],
      "metadata": {
        "id": "COU6FOS5iNKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StableAgent:\n",
        "    '''\n",
        "    An agent that attempts to fly the lander stably using a set of inflexible rules.\n",
        "    '''\n",
        "    def act(self, state):\n",
        "        '''\n",
        "        Decision making method.\n",
        "        Fly according to the rules described above.\n",
        "        '''\n",
        "        # Decision making thresholds\n",
        "        UPPER_MIN_Y = 1.5\n",
        "        LOWER_MIN_Y = 1\n",
        "        MIN_X = -0.4\n",
        "        MAX_X = 0.4\n",
        "        MIN_ANGLE = -3.14/50\n",
        "        MAX_ANGLE = 3.14/50\n",
        "\n",
        "        # Convenient forms for angle, x and y coordinates\n",
        "        x = state[0]\n",
        "        y = state[1]\n",
        "        angle = state[4]\n",
        "\n",
        "        # Avoiding magic numbers for readability\n",
        "        MAIN_ENGINE = 2\n",
        "        LEFT_ENGINE = 1\n",
        "        RIGHT_ENGINE = 3\n",
        "        DO_NOTHING = 0\n",
        "\n",
        "        # If very low, be sure to use main engine\n",
        "        if y < LOWER_MIN_Y:\n",
        "            return MAIN_ENGINE\n",
        "\n",
        "        # Try to keep angle within a small range\n",
        "        elif angle > MAX_ANGLE:\n",
        "            return RIGHT_ENGINE\n",
        "        elif angle < MIN_ANGLE:\n",
        "            return LEFT_ENGINE\n",
        "\n",
        "        # Don't stray too far left or right\n",
        "        elif x > MAX_X:\n",
        "            return LEFT_ENGINE\n",
        "        elif x < MIN_X:\n",
        "            return RIGHT_ENGINE\n",
        "\n",
        "        # If lander is stable, use main engine to maintain height\n",
        "        elif y < UPPER_MIN_Y:\n",
        "            return MAIN_ENGINE\n",
        "\n",
        "        # Else do nothing\n",
        "        else:\n",
        "            return DO_NOTHING\n",
        "\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "agent = StableAgent()\n",
        "\n",
        "play_episode(env, agent)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:08.593966Z",
          "iopub.execute_input": "2023-07-01T14:03:08.594351Z",
          "iopub.status.idle": "2023-07-01T14:03:09.113961Z",
          "shell.execute_reply.started": "2023-07-01T14:03:08.594322Z",
          "shell.execute_reply": "2023-07-01T14:03:09.112957Z"
        },
        "trusted": true,
        "id": "x_i6f9tQiNKh",
        "outputId": "79715132-0eb8-40d1-b5c1-9beb6ae5f370",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-404.82909858960977"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/Bdq1Hdl.gif)\n",
        "\n",
        "#### Observations:\n",
        "- Crafting a straightforward set of rules to guide the lunar lander is more challenging than anticipated.\n",
        "- Our initial efforts achieved some stability, but eventually, the lander lost control.\n",
        "\n",
        "---\n",
        "\n",
        "# Deep Reinforcement Learning\n",
        "To address this challenge, we'll use deep reinforcement learning techniques to train an agent to land the spacecraft.\n",
        "\n",
        "Simpler tabular methods are limited to discrete observation spaces, meaning there are a finite number of possible states. In `LunarLander-v2` however, we're dealing with a continuous range of states across 8 different parameters, meaning there are a near-infinite number of possible states. We could try to bin similar values into groups, but due to the sensitive controls of the game, even slight errors can lead to significant missteps.\n",
        "\n",
        "To get around this, we'll use a `neural network Q-function approximator`. This lets us predict the best actions to take for a given state, even when dealing with a vast number of potential states. It's a much better match for our complex landing challenge.\n",
        "\n",
        "## The DQN Algorithm:\n",
        "\n",
        "This breakthrough algorithm was used by Mihn et al in 2015 to achieve human-level performance on several Atari 2600 games.\n",
        "\n",
        "The original paper published in Nature can be viewed here:\n",
        "\n",
        "https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning\n",
        "\n",
        "The algorithm:\n",
        "\n",
        "1. **Initialization**: Begin by initializing the parameters for two neural networks, $Q(s,a)$ (referred to as the online network) and $\\hat{Q}(s,a)$ (known as the target network), with random weights. Both networks serve the function of mapping a state-action pair to a Q-value, which is an estimate of the expected return from that pair. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer to store past transition experiences.\n",
        "2. **Action Selection**: Utilize an epsilon-greedy strategy for action selection. With a probability of $\\epsilon$, select a random action $a$, but in all other instances, choose the action $a$ that maximizes the Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
        "3. **Experience Collection**: Execute the chosen action $a$ within the environment emulator and observe the resulting immediate reward $r$ and the next state $s'$.\n",
        "4. **Experience Storage**: Store the transition $(s,a,r,s')$ in the replay buffer for future reference.\n",
        "5. **Sampling**: Randomly sample a mini-batch of transitions from the replay buffer for training the online network.\n",
        "6. **Target Computation**: For every transition in the sampled mini-batch, compute the target value $y$. If the episode has ended at this step, $y$ is simply the reward $r$. Otherwise, $y$ is the sum of the reward and the discounted estimated optimal future Q-value, i.e.,  $y = r + \\gamma \\max_{a' \\in A} \\hat{Q}(s', a')$\n",
        "7. **Loss Calculation**: Compute the loss, which is the squared difference between the Q-value predicted by the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$\n",
        "8. **Online Network Update**: Update the parameters of the online network $Q(s,a)$ using Stochastic Gradient Descent (SGD) to minimize the loss.\n",
        "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network to the target network $\\hat{Q}(s,a)$.\n",
        "10. **Iterate**: Repeat the process from step 2 until convergence.\n",
        "\n",
        "### Defining the Deep Q-Network\n",
        "Our network will be a simple feedforward neural network that takes the state as input and produces Q-values for each action as output. For `LunarLander-v2` the state is an 8-dimensional vector and there are 4 possible actions.\n"
      ],
      "metadata": {
        "id": "-5hVH0tTiNKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class DQN(torch.nn.Module):\n",
        "    '''\n",
        "    This class defines a deep Q-network (DQN), a type of artificial neural network used in reinforcement learning.\n",
        "    The DQN is used to estimate the Q-values, which represent the expected return for each action in each state.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state_size: int, default=8\n",
        "        The size of the state space.\n",
        "    action_size: int, default=4\n",
        "        The size of the action space.\n",
        "    hidden_size: int, default=64\n",
        "        The size of the hidden layers in the network.\n",
        "    '''\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
        "        '''\n",
        "        Initialize a network with the following architecture:\n",
        "            Input layer (state_size, hidden_size)\n",
        "            Hidden layer 1 (hidden_size, hidden_size)\n",
        "            Output layer (hidden_size, action_size)\n",
        "        '''\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        '''\n",
        "        Define the forward pass of the DQN. This function is called when the network is called to estimate Q-values.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: torch.Tensor\n",
        "            The state for which to estimate the Q-values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The estimated Q-values for each action in the input state.\n",
        "        '''\n",
        "        x = torch.relu(self.layer1(state))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:09.916034Z",
          "iopub.execute_input": "2023-07-01T14:03:09.916401Z",
          "iopub.status.idle": "2023-07-01T14:03:11.338928Z",
          "shell.execute_reply.started": "2023-07-01T14:03:09.916372Z",
          "shell.execute_reply": "2023-07-01T14:03:11.337929Z"
        },
        "trusted": true,
        "id": "UGixcSVeiNKm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Replay Buffer\n",
        "In the context of RL, we employ a structure known as the replay buffer, which utilizes a deque. The replay buffer stores and samples experiences, which helps us overcome the problem of *step correlation*.\n",
        "\n",
        "A *deque* (double-ended queue) is a data structure that enables the addition or removal of elements from both its ends, hence the name. It is particularly useful when there is a need for fast append and pop operations from either end of the container, which it provides at O(1) time complexity. In contrast, a list offers these operations at O(n) time complexity, making the deque a preferred choice in cases that necessitate more efficient operations.\n",
        "\n",
        "Moreover, a deque allows setting a maximum size. Once this maximum size is exceeded during an insertion (push) operation at the front, the deque automatically ejects the item at the rear, thereby maintaining its maximum length.\n",
        "\n",
        "In the replay buffer, the `push` method is utilized to add an experience. If adding this experience exceeds the maximum buffer size, the oldest (rear-most) experience is automatically removed. This approach ensures that the replay buffer always contains the most recent experiences up to its capacity.\n",
        "\n",
        "The `sample` method, on the other hand, is used to retrieve a random batch of experiences from the replay buffer. This randomness is critical in breaking correlations within the sequence of experiences, which leads to more robust learning.\n",
        "\n",
        "This combination of recency and randomness allows us to learn on new training data, without training samples being highly correlated."
      ],
      "metadata": {
        "id": "pTd3Fm1ziNKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    '''\n",
        "    This class represents a replay buffer, a type of data structure commonly used in reinforcement learning algorithms.\n",
        "    The buffer stores past experiences in the environment, allowing the agent to sample and learn from them at later times.\n",
        "    This helps to break the correlation of sequential observations and stabilize the learning process.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    buffer_size: int, default=10000\n",
        "        The maximum number of experiences that can be stored in the buffer.\n",
        "    '''\n",
        "    def __init__(self, buffer_size=10000):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        '''\n",
        "        Add a new experience to the buffer. Each experience is a tuple containing a state, action, reward,\n",
        "        the resulting next state, and a done flag indicating whether the episode has ended.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The state of the environment before taking the action.\n",
        "        action: int\n",
        "            The action taken by the agent.\n",
        "        reward: float\n",
        "            The reward received after taking the action.\n",
        "        next_state: array-like\n",
        "            The state of the environment after taking the action.\n",
        "        done: bool\n",
        "            A flag indicating whether the episode has ended after taking the action.\n",
        "        '''\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Randomly sample a batch of experiences from the buffer. The batch size must be smaller or equal to the current number of experiences in the buffer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            The number of experiences to sample from the buffer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of numpy.ndarray\n",
        "            A tuple containing arrays of states, actions, rewards, next states, and done flags.\n",
        "        '''\n",
        "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.stack(states), actions, rewards, np.stack(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Get the current number of experiences in the buffer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The number of experiences in the buffer.\n",
        "        '''\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:11.340979Z",
          "iopub.execute_input": "2023-07-01T14:03:11.341626Z",
          "iopub.status.idle": "2023-07-01T14:03:11.35325Z",
          "shell.execute_reply.started": "2023-07-01T14:03:11.341588Z",
          "shell.execute_reply": "2023-07-01T14:03:11.352155Z"
        },
        "trusted": true,
        "id": "nlGkAXIBiNKp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the DQN Agent\n",
        "The DQN agent handles the interaction with the environment, selecting actions, collecting experiences, storing them in the replay buffer, and using these experiences to train the network. Let's walk through each part of this process:\n",
        "\n",
        "#### Initialisation\n",
        "The `__init__` function sets up the agent:\n",
        "\n",
        "- `self.device`: We start by checking whether a GPU is available, and, if so, we use it, otherwise, we fall back to CPU.\n",
        "- `self.gamma`: This is the discount factor for future rewards, used in the Q-value update equation.\n",
        "- `self.batch_size`: This is the number of experiences we'll sample from the memory when updating the model.\n",
        "- `self.q_network` and `self.target_network`: These are two instances of the Q-Network. The first is the network we're actively training, and the second is a copy that gets updated less frequently. This helps to stabilize learning.\n",
        "- `self.optimizer`: This is the optimization algorithm used to update the Q-Network's parameters.\n",
        "- `self.memory`: This is a replay buffer that stores experiences. It's an instance of the `ReplayBuffer` class.\n",
        "\n",
        "#### Step Function\n",
        "The `step` function is called after each timestep in the environment:\n",
        "\n",
        "- The function starts by storing the new experience in the replay buffer.\n",
        "- If enough experiences have been stored, it calls `self.update_model()`, which triggers a learning update.\n",
        "\n",
        "#### Action Selection\n",
        "The act function is how the agent selects an action:\n",
        "\n",
        "- If a randomly drawn number is greater than $\\epsilon$, it selects the action with the highest predicted Q-value. This is known as exploitation: the agent uses what it has learned to select the best action.\n",
        "- If the random number is less than $\\epsilon$, it selects an action randomly. This is known as exploration: the agent explores the environment to learn more about it.\n",
        "\n",
        "#### Model Update\n",
        "The `update_model` function is where the learning happens:\n",
        "\n",
        "- It starts by sampling a batch of experiences from the replay buffer.\n",
        "- It then calculates the current Q-values for the sampled states and actions, and the expected - Q-values based on the rewards and next states.\n",
        "- It calculates the loss, which is the mean squared difference between the current and expected Q-values.\n",
        "- It then backpropagates this loss through the Q-Network and updates the weights using the optimizer.\n",
        "\n",
        "#### Target Network Update\n",
        "Finally, the `update_target_network` function copies the weights from the Q-Network to the Target Network. This is done periodically (not every step), to stabilize the learning process. Without this, the Q-Network would be trying to follow a moving target, since it's learning from estimates produced by itself."
      ],
      "metadata": {
        "id": "1HRTZmlmiNKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    '''\n",
        "    This class represents a Deep Q-Learning agent that uses a Deep Q-Network (DQN) and a replay memory to interact\n",
        "    with its environment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    state_size: int, default=8\n",
        "        The size of the state space.\n",
        "    action_size: int, default=4\n",
        "        The size of the action space.\n",
        "    hidden_size: int, default=64\n",
        "        The size of the hidden layers in the network.\n",
        "    learning_rate: float, default=1e-3\n",
        "        The learning rate for the optimizer.\n",
        "    gamma: float, default=0.99\n",
        "        The discount factor for future rewards.\n",
        "    buffer_size: int, default=10000\n",
        "        The maximum size of the replay memory.\n",
        "    batch_size: int, default=64\n",
        "        The batch size for learning from the replay memory.\n",
        "    '''\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "        # Select device to train on (if CUDA available, use it, otherwise use CPU)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Discount factor for future rewards\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Batch size for sampling from the replay memory\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Number of possible actions\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Initialize the Q-Network and Target Network with the given state size, action size and hidden layer size\n",
        "        # Move the networks to the selected device\n",
        "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "\n",
        "        # Set weights of target network to be the same as those of the q network\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Set target network to evaluation mode\n",
        "        self.target_network.eval()\n",
        "\n",
        "        # Initialize the optimizer for updating the Q-Network's parameters\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Initialize the replay memory\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        '''\n",
        "        Perform a step in the environment, store the experience in the replay memory and potentially update the Q-network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The current state of the environment.\n",
        "        action: int\n",
        "            The action taken by the agent.\n",
        "        reward: float\n",
        "            The reward received after taking the action.\n",
        "        next_state: array-like\n",
        "            The state of the environment after taking the action.\n",
        "        done: bool\n",
        "            A flag indicating whether the episode has ended after taking the action.\n",
        "        '''\n",
        "        # Store the experience in memory\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "        # If there are enough experiences in memory, perform a learning step\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        '''\n",
        "        Choose an action based on the current state and the epsilon-greedy policy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state: array-like\n",
        "            The current state of the environment.\n",
        "        eps: float, default=0.\n",
        "            The epsilon for the epsilon-greedy policy. With probability eps, a random action is chosen.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The chosen action.\n",
        "        '''\n",
        "        # If a randomly chosen value is greater than eps\n",
        "        if random.random() > eps:\n",
        "            # Convert state to a PyTorch tensor and set network to evaluation mode\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "\n",
        "            # With no gradient updates, get the action values from the DQN\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "\n",
        "            # Revert to training mode and return action\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            # Return a random action for random value > eps\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        '''\n",
        "        Update the Q-network based on a batch of experiences from the replay memory.\n",
        "        '''\n",
        "        # Sample a batch of experiences from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        # Get Q-values for the actions that were actually taken\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Get maximum Q-value for the next states from target network\n",
        "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss between the current and expected Q values\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Zero all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        '''\n",
        "        Update the weights of the target network to match those of the Q-network.\n",
        "        '''\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:12.955828Z",
          "iopub.execute_input": "2023-07-01T14:03:12.956187Z",
          "iopub.status.idle": "2023-07-01T14:03:12.978353Z",
          "shell.execute_reply.started": "2023-07-01T14:03:12.956159Z",
          "shell.execute_reply": "2023-07-01T14:03:12.977298Z"
        },
        "trusted": true,
        "id": "6ThJybcGiNKr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Agent\n",
        "\n",
        "Training the agent involves having the agent interact with the `LunarLander-v2` environment over a sequence of steps. Over each step, the agent receives a state from the environment, selects an action, receives a reward and the next state, and then updates its understanding of the environment (the Q-table in the case of Q-Learning).\n",
        "\n",
        "The `train` function orchestrates this process over a defined number of episodes, using the methods defined in the DQNAgent class. Here's how it works:\n",
        "\n",
        "#### Initial Setup\n",
        "- `scores`: This list stores the total reward obtained in each episode.\n",
        "- `scores_window`: This is a double-ended queue with a maximum length of 100. It holds the scores of the most recent 100 episodes and is used to monitor the agent's performance.\n",
        "-`eps`: This is the epsilon for epsilon-greedy action selection. It starts from `eps_start` and decays after each episode until it reaches `eps_end`.\n",
        "\n",
        "#### Episode Loop\n",
        "The training process runs over a fixed number of episodes. In each episode:\n",
        "\n",
        "- The environment is reset to its initial state.\n",
        "- he agent then interacts with the environment until the episode is done (when a terminal state is reached).\n",
        "\n",
        "#### Step Loop\n",
        "In each step of an episode:\n",
        "\n",
        "- The agent selects an action using the current policy (the act method in `DQNAgent`).\n",
        "The selected action is applied to the environment using the step method, which returns the next state, the reward, and a boolean indicating whether the episode is done.\n",
        "- The agent's step method is called to update the agent's knowledge. This involves adding the experience to the replay buffer and, if enough experiences have been collected, triggering a learning update.\n",
        "- The state is updated to the next state, and the reward is added to the score.\n",
        "\n",
        "After each episode:\n",
        "\n",
        "- The score for the episode is added to `scores` and `scores_window`.\n",
        "- Epsilon is decayed according to `eps_decay`.\n",
        "- If the episode is a multiple of `target_update`, the target network is updated with the latest weights from the Q-Network.\n",
        "- Finally, every 100 episodes, the average score over the last 100 episodes is printed.\n",
        "\n",
        "The function returns the list of scores for all episodes.\n",
        "\n",
        "This training process, which combines experiences from the replay buffer and separate target and Q networks, helps to stabilize the learning and leads to a more robust policy."
      ],
      "metadata": {
        "id": "naQvlwmJiNKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def train(agent, env, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, target_update=10):\n",
        "    '''\n",
        "    Train a DQN agent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    agent: DQNAgent\n",
        "        The agent to be trained.\n",
        "    env: gym.Env\n",
        "        The environment in which the agent is trained.\n",
        "    n_episodes: int, default=2000\n",
        "        The number of episodes for which to train the agent.\n",
        "    eps_start: float, default=1.0\n",
        "        The starting epsilon for epsilon-greedy action selection.\n",
        "    eps_end: float, default=0.01\n",
        "        The minimum value that epsilon can reach.\n",
        "    eps_decay: float, default=0.995\n",
        "        The decay rate for epsilon after each episode.\n",
        "    target_update: int, default=10\n",
        "        The frequency (number of episodes) with which the target network should be updated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of float\n",
        "        The total reward obtained in each episode.\n",
        "    '''\n",
        "\n",
        "    # Initialize the scores list and scores window\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "\n",
        "    # Loop over episodes\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "\n",
        "        # Reset environment and score at the start of each episode\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "\n",
        "        # Loop over steps\n",
        "        while True:\n",
        "\n",
        "            # Select an action using current agent policy then apply in environment\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Update the agent, state and score\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # End the episode if done\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # At the end of episode append and save scores\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "\n",
        "        # Decrease epsilon\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "\n",
        "        # Print some info\n",
        "        print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\", end=\"\")\n",
        "\n",
        "        # Update target network every target_update episodes\n",
        "        if i_episode % target_update == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        # Print average score every 100 episodes\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "        # This environment is considered to be solved for a mean score of 200 or greater, so stop training.\n",
        "        if i_episode % 100 == 0 and np.mean(scores_window) >= 200:\n",
        "            break\n",
        "\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a DQN agent\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:20.853548Z",
          "iopub.execute_input": "2023-07-01T14:03:20.853901Z",
          "iopub.status.idle": "2023-07-01T14:03:20.866585Z",
          "shell.execute_reply.started": "2023-07-01T14:03:20.853873Z",
          "shell.execute_reply": "2023-07-01T14:03:20.865293Z"
        },
        "trusted": true,
        "id": "4Fw1zgf2iNKt",
        "outputId": "422cc945-3246-4b96-cc90-ac5b3bfc224a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: -116.73\n",
            "Episode 185\tAverage Score: -51.11"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "- Our DQN agent is able to solve the game typically after playing around 1200 episodes.\n",
        "- Let's watch a video of this agent's performance:"
      ],
      "metadata": {
        "id": "kvH_jbfSiNKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "def play_DQN_episode(env, agent):\n",
        "    score = 0\n",
        "    state, _ = env.reset(seed=42)\n",
        "\n",
        "    while True:\n",
        "        # eps=0 for predictions\n",
        "        action = agent.act(state, 0)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        score += reward\n",
        "\n",
        "        # End the episode if done\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return score\n",
        "\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "bjAYMhlJiNKu",
        "outputId": "c3acb1a8-386f-4db5-fdb5-6345684db314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-3bce00b6ab30>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_DQN_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score obtained:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-3bce00b6ab30>\u001b[0m in \u001b[0;36mplay_DQN_episode\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_DQN_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/NAg48Qk.gif)\n",
        "\n",
        "## Double DQN (DDQN)\n",
        "The Double Deep Q-Network (DDQN) algorithm is a modification of the standard Deep Q-Network (DQN) algorithm, which reduces the overestimation bias in the Q-values, thereby improving the stability of the learning process. You can read the original publication by Hasselt et al from late 2015 here:\n",
        "\n",
        "https://arxiv.org/abs/1509.06461\n",
        "\n",
        "### The DDQN Algorithm\n",
        "\n",
        "1. **Initialization**: Similar to DQN, initialize the parameters of two neural networks, $Q(s,a)$ (online network) and $\\hat{Q}(s,a)$ (target network), with random weights. Both networks estimate Q-values from state-action pairs. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer.\n",
        "\n",
        "2. **Action Selection**: Use an epsilon-greedy strategy, just like in DQN. With a probability of $\\epsilon$, select a random action $a$, otherwise, select the action $a$ that yields the highest Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
        "\n",
        "3. **Experience Collection**: Carry out the selected action $a$ in the environment to get the immediate reward $r$ and the next state $s'$.\n",
        "\n",
        "4. **Experience Storage**: Store the transition tuple $(s,a,r,s')$ in the replay buffer.\n",
        "\n",
        "5. **Sampling:** Randomly sample a mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. **Target Computation**: Here comes the primary difference from DQN. For every transition in the sampled mini-batch, compute the target value $y$. If the episode has ended, $y = r$. Otherwise, unlike DQN that uses the max operator to select the action from the target network, DDQN uses the online network to select the best action, and uses its Q-value estimate from the target network, i.e., $y = r + \\gamma \\hat{Q}(s', argmax_{a' \\in A} Q(s', a'))$. This double estimator approach helps to reduce overoptimistic value estimates.\n",
        "\n",
        "7. **Loss Calculation**: Compute the loss as the squared difference between the predicted Q-value from the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$.\n",
        "\n",
        "8. **Online Network Update**: Perform Stochastic Gradient Descent (SGD) on the online network to minimize the loss.\n",
        "\n",
        "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network.\n",
        "\n",
        "10. **Iterate**: Repeat the process from step 2 until convergence.\n",
        "\n",
        "In summary, the key difference in DDQN lies in the way the target Q-value is calculated for non-terminal states during the update. DDQN chooses the action using the online network and estimates the Q-value for this action using the target network. This modification helps mitigate the issue of overestimation present in standard DQN.\n"
      ],
      "metadata": {
        "id": "AoUMEvd7iNKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDQNAgent:\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.action_size = action_size\n",
        "        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if random.random() > eps:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        '''\n",
        "        Update the Q-network based on a batch of experiences from the replay memory.\n",
        "        '''\n",
        "        # Sample a batch of experiences from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        # Get Q-values for the actions that were actually taken\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Get the action values from the online network\n",
        "        next_action_values = self.q_network(next_states).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "        # Get the Q-values from the target network for the actions chosen by the Q-network\n",
        "        next_q_values = self.target_network(next_states).gather(1, next_action_values).detach().squeeze(-1)\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss between the current and expected Q values\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Zero all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a DQN agent\n",
        "agent = DDQNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)\n",
        "\n",
        "# Play a demonstration episode\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-01T14:03:31.999602Z",
          "iopub.execute_input": "2023-07-01T14:03:31.999979Z",
          "iopub.status.idle": "2023-07-01T14:40:49.752049Z",
          "shell.execute_reply.started": "2023-07-01T14:03:31.999942Z",
          "shell.execute_reply": "2023-07-01T14:40:49.750922Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "SdSbnIZ5iNKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/rrfB9Vl.gif)\n",
        "\n",
        "## Dueling Deep Q-Networks (Dueling DQN)\n",
        "The Dueling Deep Q-Network (Dueling DQN) algorithm is an extension of the standard Deep Q-Network (DQN) algorithm, which aims to improve the estimation of the state-value function and thus enhance the quality of the policy. The dueling architecture was proposed by Wang et al in 2015, and you can find their original paper here:\n",
        "\n",
        "https://arxiv.org/abs/1511.06581\n",
        "\n",
        "### The Dueling DQN Algorithm\n",
        "1. **Initializatin**: In Dueling DQN, initialize the parameters of two neural networks, $Q(s,a)$ (online network) and $\\hat{Q}(s,a)$ (target network), with random weights. Unlike the traditional DQN, each network in Dueling DQN splits into two separate streams at some point - one for estimating the state-value function $V(s)$ and the other for estimating the advantage function $A(s,a)$. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer.\n",
        "\n",
        "2. **Action Selection**: The action selection process is the same as DQN. Use an epsilon-greedy strategy. With a probability of $\\epsilon$, select a random action $a$, otherwise, select the action $a$ that yields the highest Q-value, i.e., $a = argmax_aQ(s,a)$.\n",
        "\n",
        "3. **Experience Collection**: Carry out the selected action $a$ in the environment to obtain the immediate reward $r$ and the next state $s'$.\n",
        "\n",
        "4. **Experience Storage**: Store the transition tuple $(s,a,r,s')$ in the replay buffer.\n",
        "\n",
        "5. **Sampling**: Randomly sample a mini-batch of transitions from the replay buffer.\n",
        "\n",
        "6. **Target Computation**: For each transition in the sampled mini-batch, compute the target value $y$. If the episode has ended, $y = r$. Otherwise, compute $y$ as $y = r + \\gamma \\hat{Q}(s', argmax_{a' \\in A} Q(s', a'))$.\n",
        "\n",
        "7. **Loss Calculation**: Compute the loss as the squared difference between the predicted Q-value from the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$.\n",
        "\n",
        "8. **Online Network Update**: Use Stochastic Gradient Descent (SGD) or another optimization algorithm to update the online network and minimize the loss.\n",
        "\n",
        "9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network.\n",
        "\n",
        "10. **Iterate**: Repeat the process from step 2 until convergence.\n",
        "\n",
        "Dueling DQN indeed introduces a novel network architecture for approximating the Q-value function. It separates the Q-value into two parts: the state-value function $V(s)$, which estimates the value of a state regardless of the actions, and the advantage function $A(s,a)$, which measures the relative advantage of taking an action in a state compared to the other actions.\n",
        "\n",
        "At first glance, it might seem logical to compute the Q-value simply by adding the state-value and the advantage: $Q(s,a) = V(s) + A(s,a)$. However, this equation presents an issue: it's underdetermined. There are infinite possible combinations of $V(s)$ and $A(s,a)$ that satisfy this equation for a given $Q(s,a)$. For instance, if the actual value of $Q(s,a)$ is 10, we would have the equation $10 = V(s) + A(s,a)$, for which there are infinite solutions.\n",
        "\n",
        "The authors of the Dueling DQN paper propose a clever way to overcome this issue: they force the advantage function to have zero advantage at the chosen action. This means that the highest advantage, $A(s,a)$, is 0, and other advantages are negative or zero, thus providing a unique solution. To implement this, they modify the equation as follows:\n",
        "\n",
        "$$ Q(s,a) = V(s)+(A(s,a) ‚àí \\max_{a'}A(s, a') $$\n",
        "\n",
        "This equation means that the Q-value is computed as the state-value $V(s)$ plus the difference between the advantage of the action $a$ and the maximum advantage over all possible actions in state $s$. In other words, the Q-value is now the value of the state plus the relative advantage of taking the action $a$ over the other actions. This mechanism provides a clear way to train the network and allows Dueling DQN to learn efficiently about state values and action advantages.\n",
        "\n",
        "To implement this, we can use the original DQN algorithm and our original DQNAgent class, we just need to change the DQN it uses, in total just 2 lines of code changes in the agent class."
      ],
      "metadata": {
        "id": "dWh0OH5JiNKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DuelingDQN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
        "\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        # Common layers\n",
        "        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Advantage layer\n",
        "        self.advantage = torch.nn.Linear(hidden_size, action_size)\n",
        "\n",
        "        # Value layer\n",
        "        self.value = torch.nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        # Common part of the network\n",
        "        x = torch.relu(self.layer1(state))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "\n",
        "        # Streams split here\n",
        "        advantage = self.advantage(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        # Recombine advantage and value for Q\n",
        "        return value + (advantage - advantage.max(dim=1, keepdim=True)[0])\n",
        "\n",
        "\n",
        "class DuelingDQNAgent:\n",
        "\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Use the dueling DQN networks instead\n",
        "        self.q_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if random.random() > eps:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a DuelingDQN agent\n",
        "agent = DuelingDQNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)\n",
        "\n",
        "# Play a demonstration episode\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "OR_NhRjziNKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.imgur.com/QNCmfd3.gif)\n",
        "\n",
        "## Double Dueling Deep Q-Networks (Dueling DQN)\n",
        "\n",
        "We can also use the DDQN training trick to prevent the overestimation of Q-values from Dueling DQN. We can call this algorithm Dueling Double Deep Q-Network, or D3QN.\n",
        "\n",
        "To use this, we just need to change the code in our DuelingDQN agent's `update_model` method so it uses the DDQN trick to prevent Q-value overestimation:"
      ],
      "metadata": {
        "id": "jhWppSdGiNKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class D3QNAgent:\n",
        "    def __init__(self, state_size=8, action_size=4, hidden_size=64,\n",
        "                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.action_size = action_size\n",
        "        self.q_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network = DuelingDQN(state_size, action_size, hidden_size).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            self.update_model()\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        if random.random() > eps:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "            self.q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "\n",
        "            self.q_network.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def update_model(self):\n",
        "        '''\n",
        "        Update the Q-network based on a batch of experiences from the replay memory.\n",
        "        '''\n",
        "        # Sample a batch of experiences from memory\n",
        "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        states = torch.from_numpy(states).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n",
        "        next_states = torch.from_numpy(next_states).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        # Get Q-values for the actions that were actually taken\n",
        "        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Get the action values from the online network\n",
        "        next_action_values = self.q_network(next_states).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "        # Get the Q-values from the target network for the actions chosen by the Q-network\n",
        "        next_q_values = self.target_network(next_states).gather(1, next_action_values).detach().squeeze(-1)\n",
        "\n",
        "        # Compute the expected Q-values\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        # Compute the loss between the current and expected Q values\n",
        "        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Zero all gradients\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Step the optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        '''\n",
        "        Update the weights of the target network to match those of the Q-network.\n",
        "        '''\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Make an environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initilize a D3QN agent\n",
        "agent = D3QNAgent(state_size, action_size)\n",
        "\n",
        "# Train it\n",
        "scores = train(agent, env)\n",
        "\n",
        "# Play a demonstration episode\n",
        "score = play_DQN_episode(env, agent)\n",
        "print(\"Score obtained:\", score)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "id": "TS5Ut_p4iNKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}